{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import  numpy as np\n",
    "\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', is_slippery=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зима пришла. Вы и ваши друзья бросали фрисби в парке, когда вы сделали дикий бросок, который оставил фрисби посреди озера. Вода в основном замерзла, но есть несколько лунок, где лед растаял. Если вы войдете в одну из этих дыр, вы упадете в ледяную воду. В настоящее время существует нехватка международных фрисби, поэтому абсолютно необходимо, чтобы вы пересекли озеро и забрали диск. Однако лед скользкий, поэтому вы не всегда будете двигаться в том направлении, в котором хотите.\n",
    "\n",
    "Эпизод заканчивается, когда вы достигаете цели или падаете в яму. Вы получаете вознаграждение в размере 1, если достигаете цели, и ноль в противном случае."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_state = env.reset()\n",
    "print(init_state)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обозначения\n",
    "<pre class=\"literal-block\">SFFF       (S: starting point, safe)\n",
    "FHFH       (F: frozen surface, safe)\n",
    "FFFH       (H: hole, fall to your doom)\n",
    "HFFG       (G: goal, where the frisbee is located)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Действия:\n",
    "<pre class=\"literal-block\">\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_to_symbol = {\n",
    "    0: '\\u2190',\n",
    "    1: '\\u2193',\n",
    "    2: '\\u2192',\n",
    "    3: '\\u2191'\n",
    "}\n",
    "for a in action_to_symbol.keys():\n",
    "    print(a,':', action_to_symbol[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = env.action_space.sample()\n",
    "print(\"action=\",action)\n",
    "next_state, reward, done, info = env.step(action)\n",
    "print(\"next_state, reward, done =\",next_state, reward, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_state = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, info =env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = 8\n",
    "action = 2\n",
    "print(env.unwrapped.P[state][action])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre class=\"literal-block\">\n",
    "`is_slippery`: True/False. If True will move in intended direction with\n",
    "probability of 1/3 else will move in either perpendicular direction with\n",
    "equal probability of 1/3 in both directions.\n",
    "    For example, if action is left and is_slippery is True, then:\n",
    "    - P(move left)=1/3\n",
    "    - P(move up)=1/3\n",
    "    - P(move down)=1/3\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP():\n",
    "    def __init__(self, env):\n",
    "        self.states = np.arange(env.observation_space.n)\n",
    "        self.actions = np.arange(env.action_space.n)\n",
    "        self.P = env.unwrapped.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = MDP(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mdp.states)\n",
    "print(mdp.actions)\n",
    "state = 8\n",
    "action = 2\n",
    "print(mdp.P[state][action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs, next_states, rewards, dones = zip(*mdp.P[state][action])\n",
    "probs       = np.array(probs, dtype=np.float32)\n",
    "next_states = np.array(next_states, dtype=np.int32)\n",
    "rewards     = np.array(rewards, dtype=np.float32)\n",
    "dones       = np.array(dones, dtype=np.float32)\n",
    "probs, next_states, rewards, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = np.zeros(len(mdp.states), dtype=np.float32)\n",
    "q_value = np.sum(probs*(rewards + (1-dones)*V[next_states]))\n",
    "q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_q_value(transitions, V):\n",
    "    probs, next_states, rewards, dones = zip(*transitions)\n",
    "    probs       = np.array(probs, dtype=np.float32)\n",
    "    next_states = np.array(next_states, dtype=np.int32)\n",
    "    rewards     = np.array(rewards, dtype=np.float32)\n",
    "    dones       = np.array(dones, dtype=np.float32)\n",
    "    q_value = np.sum(probs*(rewards + (1-dones)*V[next_states]))\n",
    "    return q_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DYNAMIC PROGRAMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = np.zeros(len(mdp.states), dtype=np.float64)\n",
    "steps_number = 100\n",
    "for _ in np.arange(steps_number):\n",
    "    V_next  = np.zeros(len(mdp.states), dtype=np.float64)\n",
    "    for s in  mdp.states:\n",
    "        q_values = []\n",
    "        for a in mdp.actions:\n",
    "            q_value = compute_q_value(mdp.P[s][a], V)\n",
    "            q_values.append(q_value)\n",
    "        V_next[s] = np.max(q_values)\n",
    "    V = V_next\n",
    "    print(V.reshape((4,4)))               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VALUE ITERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_q_value(transitions, V, gamma):\n",
    "    probs, next_states, rewards, dones = zip(*transitions)\n",
    "    probs       = np.array(probs, dtype=np.float32)\n",
    "    next_states = np.array(next_states, dtype=np.int32)\n",
    "    rewards     = np.array(rewards, dtype=np.float32)\n",
    "    dones       = np.array(dones, dtype=np.float32)\n",
    "    q_value = np.sum(probs*(rewards + gamma*(1-dones)*V[next_states]))\n",
    "    return q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "V = np.zeros(len(mdp.states), dtype=np.float64)\n",
    "epsilon = 1e-10\n",
    "gamma = 0.999\n",
    "#--------------------------------------------------------------------------\n",
    "steps_number = 0\n",
    "while True:\n",
    "    V_next  = np.zeros(len(mdp.states), dtype=np.float64)\n",
    "    for s in  mdp.states:\n",
    "        q_values = []\n",
    "        for a in mdp.actions:\n",
    "            q_value = compute_q_value(mdp.P[s][a], V, gamma)\n",
    "            q_values.append(q_value)\n",
    "        V_next[s] = np.max(q_values)\n",
    "    error = np.max(np.abs(V - V_next))\n",
    "    errors.append(error)\n",
    "    V = V_next\n",
    "    steps_number+=1\n",
    "    if error<=epsilon:\n",
    "        break\n",
    "print(steps_number)\n",
    "print(V.reshape((4,4)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нахождение оптимальной стратегии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q_fn(mdp, V, gamma):\n",
    "    \"\"\"\n",
    "    return function (state * action-> q_value)\n",
    "    \"\"\"\n",
    "    return lambda state,action: compute_q_value(mdp.P[state][action], V, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy(mdp, V, gamma):\n",
    "    \"\"\"\n",
    "    return function (state -> action)\n",
    "    \"\"\"\n",
    "    q_fn  = get_q_fn(mdp, V, gamma)\n",
    "    return lambda state: np.argmax( [q_fn(state, action) for action in mdp.actions])\n",
    "\n",
    "policy = get_policy(mdp, V, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_actions = np.array([ policy(s) for s in mdp.states  ])\n",
    "best_actions.reshape( (4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "env.render()\n",
    "print('Решение:')\n",
    "print(np.vectorize(action_to_symbol.get)(best_actions.reshape( (4,4))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POLICY ITERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_policy_dict = {s: np.random.choice(mdp.actions) for s in mdp.states} \n",
    "init_policy = lambda s: init_policy_dict[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([init_policy(s) for s in mdp.states])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval(policy, mdp, gamma, epsilon):\n",
    "    V = np.zeros(len(mdp.states), dtype=np.float64)\n",
    "    errors = []\n",
    "    #--------------------------------------------------------------------------\n",
    "    steps_number = 0\n",
    "    while True:\n",
    "        V_next  = np.zeros(len(mdp.states), dtype=np.float64)\n",
    "        for s in  mdp.states:\n",
    "            a = policy(s)\n",
    "            q_value = compute_q_value(mdp.P[s][a], V, gamma)\n",
    "            V_next[s] = q_value\n",
    "        error = np.max(np.abs(V - V_next))\n",
    "        errors.append(error)\n",
    "        V = V_next\n",
    "        steps_number+=1\n",
    "        if error<=epsilon:\n",
    "            break\n",
    "    print(\"Steps=\", steps_number)\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(V, mdp, policy):\n",
    "    new_policy = get_policy(mdp, V, gamma)\n",
    "    return new_policy    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = policy_eval(policy, mdp, 0.99, 0.1**10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_policy_dict = {s: np.random.choice(mdp.actions) for s in mdp.states} \n",
    "init_policy = lambda s: init_policy_dict[s]\n",
    "print([init_policy(s) for s in mdp.states])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = init_policy\n",
    "while True:\n",
    "    V = policy_eval(policy, mdp, 0.99, 0.1**10)\n",
    "    new_policy = policy_improvement(V, mdp, policy)\n",
    "    \n",
    "    policy_actions= [policy(s) for s in mdp.states]\n",
    "    new_policy_actions= [new_policy(s) for s in mdp.states]\n",
    "    policy = new_policy\n",
    "    if (policy_actions==new_policy_actions):\n",
    "        break   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_actions = np.array([ policy(s) for s in mdp.states  ])\n",
    "best_actions.reshape( (4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "env.render()\n",
    "print('Решение:')\n",
    "print(np.vectorize(action_to_symbol.get)(best_actions.reshape( (4,4))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gametheory]",
   "language": "python",
   "name": "conda-env-gametheory-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
